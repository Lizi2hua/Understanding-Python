# 激活函数

## 1. 激活函数

​		激活函数**给神经网络引入了非线性的能力，满足连续可导（条件性满足）。**

### 1.1 Introduction

​		如果没有激活函数：
$$
f_{11}(x_1,x1_2)=0.3*x_1+0.4*x_2+3\\
f_{12}(x_1,x_2)=0.6*x_1+0.6*x_2+2\\
f_{21}(f_11,f_12)=0.5*f_11+0.6f_12+3
$$
​		得到得图像是一个超平面，无法解决线性不可分问题。

<img src="C:/Users/Administrator/Desktop/Project：777/CODE/python/Note/src/linearModel_withoutActFunc.jpg" alt="linearModel_withoutActFunc" style="zoom:50%;" />



​		加了激活函数的图像，使用sigmoid饱和函数激活：

<img src="C:/Users/Administrator/Desktop/Project：777/CODE/python/Note/src/linear_withActFunc.jpg" alt="linear_withActFunc" style="zoom: 50%;" />



使用的工具[^2]



### 1.2 一些常用的激活函数

### 1.2.1 梯度消失/梯度弥散的问题

​		sigmoid函数是一类**logistic函数，即不管输入是什么，得到的输出都在0到1之间。**
$$
sigmoid(x)=\frac{1}{1+e^{-x}}
$$
<img src="C:/Users/Administrator/Desktop/Project：777/CODE/python/Note/src/sigmoid.jpg" alt="sigmoid" style="zoom:67%;" />

sigmoid这样的函数通常被成为**非线性函数，因为我们不能用线性的项来描述它。**很多激活函数都是非线性或者线性函数的组合。**在求梯度时候，sigmoid的导数为：**
$$
\frac{\partial{sigmoid}}{\part{x}}=\frac{e^{-x}}{(e^{-x}+1)^2}
$$
<img src="C:/Users/Administrator/Desktop/Project：777/CODE/python/Note/src/partial_sigmoid.jpg" alt="partial_sigmoid" style="zoom:50%;" />

由此可见，**当梯度很大的时候，经过sigmoid函数后梯度将会变得接几乎为0。根据参数更新算法，梯度为0意味着不更新。**如果网络中有太多的这种情况则意味着网络只有细微的更新，网络就不会有多大的改善。

#### 1.2.2 ReLU及变种

​		**ReLU,** *** Rectified Linear Unit,整流线性单元。***
$$
ReLU=max(0,x)
$$
优点：ReLU延缓了梯度消失的问题，且由于其线性特点，训练快很多。

缺点：如果$x\leq0$，那么ReLU输出为0，梯度也为0，权重将不再更新，导致节点不再学习。

​		**Leaky ReLU**
$$
Leaky \ ReLU =max(0.1x,x)
$$

​	